{
  "text": "```python\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nimport json\nimport numpy as np\nimport base64\nimport io\n\ndef scrape_and_process_wikipedia(url, primary_column_keywords, secondary_column_keywords=None):\n    \"\"\"\n    Scrapes tables from a Wikipedia URL, processes them, and returns a DataFrame.\n\n    Args:\n        url (str): The URL of the Wikipedia page.\n        primary_column_keywords (dict): A dictionary where keys are desired column names\n                                        (e.g., 'country', 'population') and values are\n                                        lists of keywords to match in the HTML table headers.\n                                        All primary columns must be found for a table to be considered.\n        secondary_column_keywords (dict, optional): Similar to primary_column_keywords,\n                                                     but for less critical columns.\n                                                     These add to the score but are not mandatory.\n\n    Returns:\n        pandas.DataFrame: A cleaned and processed DataFrame, or an empty DataFrame\n                          with a warning if no suitable table is found or an error occurs.\n    \"\"\"\n    try:\n        # Use html5lib for robustness with malformed HTML\n        tables = pd.read_html(url, flavor='html5lib')\n    except Exception as e:\n        print(f\"Error reading HTML from {url}: {e}\")\n        return pd.DataFrame()\n\n    best_match_df = pd.DataFrame()\n    best_match_score = -1\n    matched_column_map = {} # Maps desired_col_name -> actual_normalized_col_name\n\n    for i, df in enumerate(tables):\n        current_df_columns = df.columns.tolist()\n        normalized_columns = []\n        \n        # Normalize column names\n        for col in current_df_columns:\n            # Handle MultiIndex columns if present (e.g., from nested headers)\n            if isinstance(col, tuple):\n                # Join tuple elements, remove NaNs, then normalize\n                normalized_col_name = ' '.join(filter(None, [str(c) for c in col])).strip()\n            else:\n                normalized_col_name = str(col)\n\n            normalized_col_name = re.sub(r'\\[\\d+\\]', '', normalized_col_name) # Remove footnotes like [1], [2]\n            normalized_col_name = normalized_col_name.replace('\\n', ' ').strip().lower() # Remove newlines, strip, lowercase\n            normalized_columns.append(normalized_col_name)\n\n        df.columns = normalized_columns\n\n        # Check for primary column matches\n        current_score = 0\n        temp_column_map = {}\n        all_primary_found = True\n        for desired_col, keywords in primary_column_keywords.items():\n            found_col = None\n            for keyword in keywords:\n                # Use regex for partial matching, e.g., 'country' matches 'country (territory)'\n                # Ensure it's a whole word match for the keyword to avoid matching 'pop' in 'popular'\n                matching_cols = [col for col in normalized_columns if re.search(r'\\b' + re.escape(keyword) + r'\\b', col)]\n                if matching_cols:\n                    found_col = matching_cols[0] # Take the first match\n                    break\n            if found_col:\n                current_score += 1\n                temp_column_map[desired_col] = found_col\n            else:\n                all_primary_found = False\n                break # A primary column is missing, this table is not suitable\n\n        if not all_primary_found:\n            continue # Skip this table if any primary column is missing\n\n        # Add score for secondary columns\n        if secondary_column_keywords:\n            for desired_col, keywords in secondary_column_keywords.items():\n                found_col = None\n                for keyword in keywords:\n                    matching_cols = [col for col in normalized_columns if re.search(r'\\b' + re.escape(keyword) + r'\\b', col)]\n                    if matching_cols:\n                        found_col = matching_cols[0]\n                        break\n                if found_col:\n                    current_score += 0.5 # Give less weight to secondary columns\n                    temp_column_map[desired_col] = found_col\n\n        # Choose the table with the highest score (most relevant columns)\n        if current_score > best_match_score:\n            best_match_score = current_score\n            best_match_df = df.copy()\n            matched_column_map = temp_column_map\n\n    if best_match_df.empty:\n        print(\"Warning: No suitable table found on the page matching the specified column keywords.\")\n        return pd.DataFrame()\n\n    # Rename columns to desired names for consistency\n    # Only rename columns that were actually matched and will be kept\n    columns_to_rename = {v: k for k, v in matched_column_map.items()}\n    best_match_df = best_match_df.rename(columns=columns_to_rename)\n\n    # Data Cleaning\n    # Iterate through the columns that were successfully matched and renamed\n    for desired_col in matched_column_map.keys():\n        if desired_col in best_match_df.columns: # Ensure the column exists after renaming\n            # Check if the column is likely numeric based on its desired name\n            is_numeric_col = any(k in desired_col for k in ['gross', 'rank', 'peak', 'year', 'population', 'area', 'gdp', 'value', 'number', 'density'])\n\n            if is_numeric_col:\n                # Clean numeric columns\n                best_match_df[desired_col] = best_match_df[desired_col].astype(str)\n                # Remove commas, spaces, currency symbols, and footnotes, keep only digits and a single decimal point\n                # Handle cases like \"$2,789,700,000[1]\" -> \"2789700000\"\n                best_match_df[desired_col] = best_match_df[desired_col].apply(\n                    lambda x: re.sub(r'[^\\d.]', '', x) if pd.notna(x) else x\n                )\n                # Convert to numeric, coercing errors to NaN\n                best_match_df[desired_col] = pd.to_numeric(best_match_df[desired_col], errors='coerce')\n                # Drop rows where primary numeric columns are NaN (only if it's a primary column)\n                if desired_col in primary_column_keywords:\n                    best_match_df.dropna(subset=[desired_col], inplace=True)\n                \n                # Convert to int if possible and if it makes sense (e.g., year, rank)\n                if desired_col in ['year', 'rank', 'peak'] and pd.api.types.is_numeric_dtype(best_match_df[desired_col]):\n                    # Convert to Int64 to allow NaN values\n                    best_match_df[desired_col] = best_match_df[desired_col].astype('Int64')\n            else:\n                # Clean text columns\n                best_match_df[desired_col] = best_match_df[desired_col].astype(str)\n                best_match_df[desired_col] = best_match_df[desired_col].apply(\n                    lambda x: re.sub(r'\\[\\d+\\]', '', x).strip() if pd.notna(x) else x\n                )\n                # Remove rows where text columns are empty or 'nan' after cleaning\n                best_match_df.replace('', pd.NA, inplace=True)\n                best_match_df.replace('nan', pd.NA, inplace=True) # Handle string 'nan'\n                best_match_df.dropna(subset=[desired_col], inplace=True)\n\n    # Ensure only the desired columns are returned, in the order specified by primary_column_keywords\n    # followed by secondary_column_keywords\n    final_columns_order = list(primary_column_keywords.keys())\n    if secondary_column_keywords:\n        final_columns_order.extend(list(secondary_column_keywords.keys()))\n\n    # Filter to only include columns that were actually matched and processed\n    final_columns_present = [col for col in final_columns_order if col in best_match_df.columns]\n\n    return best_match_df[final_columns_present]\n\n# --- Main execution for the specific questions ---\nurl = \"https://en.wikipedia.org/wiki/List_of_highest-grossing_films\"\n\nprimary_cols = {\n    'film': ['film', 'title', 'movie'],\n    'gross': ['gross', 'worldwide gross', 'revenue', 'box office'],\n    'year': ['year', 'release year', 'released']\n}\n\nsecondary_cols = {\n    'rank': ['rank'],\n    'peak': ['peak']\n}\n\ndf = scrape_and_process_wikipedia(url, primary_cols, secondary_cols)\n\nanswers = []\n\nif not df.empty:\n    # Ensure 'gross', 'year', 'rank', 'peak' are numeric\n    df['gross'] = pd.to_numeric(df['gross'], errors='coerce')\n    df['year'] = pd.to_numeric(df['year'], errors='coerce')\n    df['rank'] = pd.to_numeric(df['rank'], errors='coerce')\n    df['peak'] = pd.to_numeric(df['peak'], errors='coerce')\n\n    df.dropna(subset=['gross', 'year', 'rank', 'peak'], inplace=True)\n\n    # Question 1: How many $2 bn movies were released before 2000?\n    q1_result = df[(df['gross'] >= 2_000_000_000) & (df['year'] < 2000)]\n    num_2bn_before_2000 = len(q1_result)\n    answers.append(f\"Number of $2 billion movies released before 2000: {num_2bn_before_2000}\")\n\n    # Question 2: Which is the earliest film that grossed over $1.5 bn?\n    q2_result = df[df['gross'] >= 1_500_000_000].sort_values(by='year').reset_index(drop=True)\n    earliest_film_1_5bn = \"N/A\"\n    if not q2_result.empty:\n        earliest_film_1_5bn = q2_result.iloc[0]['film']\n    answers.append(f\"Earliest film that grossed over $1.5 billion: {earliest_film_1_5bn}\")\n\n    # Question 3: What's the correlation between the Rank and Peak?\n    correlation = \"N/A\"\n    if 'rank' in df.columns and 'peak' in df.columns and not df[['rank', 'peak']].dropna().empty:\n        correlation = df['rank'].corr(df['peak'])\n        answers.append(f\"Correlation between Rank and Peak: {correlation:.4f}\")\n    else:\n        answers.append(\"Correlation between Rank and Peak: Not enough data or columns missing.\")\n\n    # Question 4: Draw a scatterplot of Rank and Peak along with a dotted red regression line through it.\n    # Return as a base-64 encoded data URI, \"data:image/png;base64,iVBORw0...\" under 100,000 bytes.\n    scatterplot_uri = \"N/A\"\n    if 'rank' in df.columns and 'peak' in df.columns and not df[['rank', 'peak']].dropna().empty:\n        try:\n            plt.figure(figsize=(6, 4), dpi=80) # Adjust figsize and dpi for size constraint\n            plt.scatter(df['rank'], df['peak'], alpha=0.7)\n            plt.title('Rank vs. Peak Grossing Films')\n            plt.xlabel('Rank')\n            plt.ylabel('Peak')\n            plt.grid(True, linestyle='--', alpha=0.6)\n\n            # Add regression line\n            # Filter out NaN values for polyfit\n            valid_data = df[['rank', 'peak']].dropna()\n            if not valid_data.empty:\n                x = valid_data['rank']\n                y = valid_data['peak']\n                m, b = np.polyfit(x, y, 1) # Calculate slope (m) and intercept (b)\n                plt.plot(x, m*x + b, color='red', linestyle=':', label=f'Regression Line (y={m:.2f}x+{b:.2f})')\n                plt.legend()\n\n            # Save plot to a BytesIO object\n            buf = io.BytesIO()\n            plt.savefig(buf, format='png', bbox_inches='tight')\n            buf.seek(0)\n            \n            # Encode to base64\n            scatterplot_uri = \"data:image/png;base64,\" + base64.b64encode(buf.read()).decode('utf-8')\n            buf.close()\n            plt.close() # Close the plot to free memory\n            answers.append(scatterplot_uri)\n        except Exception as e:\n            answers.append(f\"Error generating scatterplot: {e}\")\n    else:\n        answers.append(\"Scatterplot: Not enough data or columns missing for Rank and Peak.\")\n\nelse:\n    answers.append(\"Failed to retrieve any data from the Wikipedia page.\")\n\nprint(json.dumps(answers))\n```"
}